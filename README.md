# üìö Implementaci√≥n de Algoritmos de Machine Learning Desde Cero

Este repositorio es un viaje educativo a trav√©s de los fundamentos del **aprendizaje autom√°tico supervisado**. A diferencia de los enfoques convencionales que utilizan librer√≠as de alto nivel como `scikit-learn`, este proyecto se enfoca en la implementaci√≥n de los algoritmos m√°s comunes **desde cero**, utilizando √∫nicamente herramientas esenciales como `pandas` y `NumPy` como bases.

El objetivo principal es ir m√°s all√° del "c√≥mo usar" y sumergirse en el "c√≥mo funciona", traduciendo la teor√≠a matem√°tica y los conceptos estad√≠sticos directamente en c√≥digo funcional. Este enfoque permite una comprensi√≥n profunda de las fortalezas y debilidades de cada modelo, as√≠ como una visi√≥n clara de los procesos de optimizaci√≥n que tienen lugar "bajo el cap√≥".

---

## üõ†Ô∏è Metodolog√≠a y Componentes del Modelo

Cada algoritmo en este proyecto ha sido dise√±ado como una clase modular e intuitiva, reflejando las mejores pr√°cticas de la programaci√≥n orientada a objetos.

### 1. Enfoque General de Implementaci√≥n

Para cada modelo, se siguen estos principios:

- **Fundamentos Matem√°ticos**: La implementaci√≥n del c√≥digo se basa directamente en las f√≥rmulas de optimizaci√≥n, funciones de costo y teoremas que rigen el algoritmo.
- **Entrenamiento (`fit`)**: Un m√©todo clave que se encarga de todo el proceso de aprendizaje del modelo a partir de los datos.
- **Predicci√≥n (`predict`)**: Un m√©todo que utiliza los par√°metros aprendidos para hacer predicciones sobre nuevas instancias.
- **Modularidad**: Cada algoritmo est√° encapsulado en su propia clase, haciendo el c√≥digo limpio y f√°cil de reutilizar.

---

### 2. Algoritmos Implementados

Este proyecto cubre un espectro de modelos fundamentales de aprendizaje supervisado para tareas de **regresi√≥n** y **clasificaci√≥n**.

#### Modelos de Regresi√≥n

- **Regresi√≥n Lineal Simple y M√∫ltiple**: Se enfoca en predecir un valor continuo al encontrar la relaci√≥n lineal √≥ptima entre una o m√∫ltiples variables de entrada y una variable de salida. La implementaci√≥n se centra en la minimizaci√≥n del **Error Cuadr√°tico Medio (MSE)** a trav√©s de **descenso de gradiente**.

#### Modelos de Clasificaci√≥n

- **Regresi√≥n Log√≠stica**: Aunque su nombre contiene la palabra "regresi√≥n", este modelo se utiliza para la **clasificaci√≥n binaria**. A diferencia de la regresi√≥n lineal, utiliza una funci√≥n **sigmoide** para transformar la salida en una probabilidad, lo que permite clasificar las instancias en dos clases distintas. La optimizaci√≥n se realiza minimizando la **funci√≥n de p√©rdida de entrop√≠a cruzada**.

- **K-Vecinos M√°s Cercanos (KNN)**: Un algoritmo **no param√©trico** y de "aprendizaje perezoso". En lugar de aprender un modelo, simplemente almacena los datos de entrenamiento y clasifica una nueva instancia bas√°ndose en la mayor√≠a de votos de sus 'K' vecinos m√°s cercanos, calculando la **distancia** entre puntos.

- **√Årboles de Decisi√≥n y Bosques Aleatorios**:

  - **√Årboles de Decisi√≥n**: Construye una estructura de "si-entonces" para tomar decisiones, dividiendo los datos en ramas basadas en las caracter√≠sticas m√°s informativas.
  - **Bosques Aleatorios**: Un **m√©todo de ensemble** que mejora la precisi√≥n y la robustez al combinar los resultados de m√∫ltiples √°rboles de decisi√≥n entrenados de forma independiente.

- **M√°quinas de Vectores de Soporte (SVM)**: Un poderoso algoritmo que busca el **hiperplano** que maximiza el margen de separaci√≥n entre las clases. La implementaci√≥n se centra en resolver este **problema de optimizaci√≥n** para encontrar los par√°metros `w` y `b` √≥ptimos.

- **Naive Bayes**: Un clasificador **probabil√≠stico** basado en el **Teorema de Bayes**. Su nombre se debe a la suposici√≥n "ingenua" de que todas las caracter√≠sticas son independientes entre s√≠, lo que lo hace computacionalmente muy eficiente y efectivo, especialmente en la clasificaci√≥n de texto. La implementaci√≥n incluye una t√©cnica crucial: el **suavizado de Laplace** para manejar de manera robusta los datos que el modelo no ha visto previamente.

---

## üß† Conceptos Fundamentales Cubiertos

Al implementar estos modelos desde cero, se adquiere una comprensi√≥n profunda de los siguientes conceptos:

- **Descenso de Gradiente**: El motor de optimizaci√≥n detr√°s de los modelos de regresi√≥n y SVM, que ajusta iterativamente los par√°metros para minimizar una funci√≥n de p√©rdida.
- **Funciones de P√©rdida**: M√©tricas como el **Error Cuadr√°tico Medio**, la **Entrop√≠a Cruzada** y la **Hinge Loss** que cuantifican el error del modelo y gu√≠an el proceso de aprendizaje.
- **Vectorizaci√≥n**: El uso de operaciones matriciales de `NumPy` para realizar c√°lculos eficientes y evitar bucles lentos en Python.
- **Programaci√≥n Orientada a Objetos (POO)**: La organizaci√≥n del c√≥digo en clases modulares y reutilizables, lo que mejora la claridad y la escalabilidad.
- **Manejo de Datos Categ√≥ricos y Num√©ricos**: C√≥mo los diferentes algoritmos abordan distintos tipos de datos.

---

## üöÄ Conclusi√≥n

Este proyecto no es solo una colecci√≥n de scripts; es una demostraci√≥n de una comprensi√≥n s√≥lida de los principios te√≥ricos y pr√°cticos del machine learning. Es un recurso valioso para aquellos que buscan consolidar sus conocimientos, entender c√≥mo funcionan los modelos m√°s all√° de las librer√≠as, y construir una base robusta para enfrentar problemas de datos m√°s complejos.

---

## üõ†Ô∏è Requisitos de Instalaci√≥n

Para ejecutar este proyecto, solo necesitas tener instaladas los requerimientos:

```bash
pip install -r requirements.txt
```
