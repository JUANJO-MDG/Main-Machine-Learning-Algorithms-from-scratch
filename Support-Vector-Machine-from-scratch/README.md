# ‚öîÔ∏è Implementaci√≥n Desde Cero de una M√°quina de Vectores de Soporte (SVM)

Este proyecto es una implementaci√≥n **completa y did√°ctica** de una **M√°quina de Vectores de Soporte (SVM)** desde cero, utilizando √∫nicamente `NumPy` para el c√°lculo matricial y `Matplotlib/Seaborn` para la visualizaci√≥n.  
A diferencia de las librer√≠as de alto nivel como `scikit-learn`, aqu√≠ se construye paso a paso el algoritmo, entendiendo c√≥mo funcionan los componentes internos.

El proyecto cubre todas las etapas esenciales: preprocesamiento de etiquetas, definici√≥n de la funci√≥n de p√©rdida (Hinge Loss), c√°lculo de gradientes, descenso de gradiente para optimizar los par√°metros y visualizaci√≥n del error durante el entrenamiento.

---

## üõ†Ô∏è Metodolog√≠a y Componentes del Modelo

El n√∫cleo del proyecto es la clase `SVM`, dise√±ada para ser **modular, intuitiva y cercana a la formulaci√≥n matem√°tica**.

### 1. Implementaci√≥n de la Clase `SVM`

La clase `SVM` consta de varios m√©todos clave:

- **Inicializaci√≥n (`__init__`)**  
  Define los hiperpar√°metros principales:
  - `lr`: Tasa de aprendizaje para el descenso de gradiente.
  - `lambda_param`: Par√°metro de regularizaci√≥n (\(\lambda\)) para controlar el sobreajuste.
  - `n_iters`: N√∫mero de iteraciones de entrenamiento.

---

### 2. Funciones Internas

- **C√°lculo de Scores (`_scores`)**  
  Implementa la funci√≥n de decisi√≥n:  
  \[
  f(x) = w \cdot x + b
  \]  
  donde `w` es el vector de pesos y `b` el sesgo.

- **Funci√≥n de P√©rdida Hinge (`_hinge_loss`)**  
  Define el costo que gu√≠a el aprendizaje:  
  \[
  L(w, b) = \frac{1}{n}\sum\_{i=1}^n \max(0, 1 - y_i f(x_i)) + \frac{\lambda}{2}\|w\|^2
  \]
  - El t√©rmino \(\max(0, 1 - y_i f(x_i))\) penaliza las muestras mal clasificadas o cercanas al margen.
  - El t√©rmino \(\frac{\lambda}{2}\|w\|^2\) es la regularizaci√≥n que evita que los pesos crezcan demasiado.

---

### 3. Entrenamiento (`fit`)

El entrenamiento sigue los pasos del **descenso de gradiente**:

1. Conversi√≥n de etiquetas ‚Üí aseguramos que sean \(-1\) y \(+1\).
2. Inicializaci√≥n de par√°metros en cero (`w`, `b`).
3. En cada iteraci√≥n:
   - Se calculan los _scores_ y los _margins_.
   - Se eval√∫a la funci√≥n de p√©rdida y se guarda en el historial.
   - Se calculan los gradientes de \(w\) y \(b\).
   - Se actualizan los par√°metros restando el gradiente escalado por la tasa de aprendizaje.

Esto refleja la optimizaci√≥n de la funci√≥n de costo hasta converger.

---

### 4. Visualizaci√≥n del Entrenamiento (`plot_loss`)

Permite graficar c√≥mo disminuye la p√©rdida a lo largo de las iteraciones, verificando que el modelo efectivamente aprende.

---

## üìä Resultados y Evaluaci√≥n del Modelo

- El modelo aprende correctamente a separar los datos al optimizar los par√°metros \(w\) y \(b\).
- La gr√°fica de _loss_ demuestra la convergencia progresiva de la funci√≥n de costo.
- El modelo puede generalizar a datos de prueba, alcanzando precisiones muy altas en datasets simples.

Ejemplo t√≠pico de desempe√±o:

- **Precisi√≥n en entrenamiento**: ~100%
- **Precisi√≥n en test**: ~100% (en datasets sint√©ticos linealmente separables).

---

## üöÄ Conclusi√≥n

Este proyecto muestra c√≥mo una SVM puede implementarse desde cero de manera clara y did√°ctica, conectando directamente las f√≥rmulas matem√°ticas con su traducci√≥n a c√≥digo.  
Al comprender el proceso paso a paso, se logra una visi√≥n profunda del algoritmo y sus fundamentos en optimizaci√≥n.

---

## üõ†Ô∏è Requisitos de Instalaci√≥n

Para ejecutar este proyecto, instala los `requirements.txt`:

```bash
pip install -r requirements.txt
```
